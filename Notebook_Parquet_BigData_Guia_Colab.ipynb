{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Archivos `.PARQUET` en Analítica y Big Data (Guía + Demo en Google Colab)\n\nEste notebook sirve para **explicar qué es Parquet**, compararlo con **XLSX/CSV** y **leer un archivo `.parquet`** para generar un **reporte automático de variables** (tipos, nulos, ejemplos y estadísticos).\n\n---\n\n## Objetivos de aprendizaje\nAl finalizar, podrás:\n\n1. Explicar **qué es** un archivo Parquet y **por qué se usa** en Big Data.\n2. Comparar Parquet vs **XLSX** vs **CSV** (ventajas, limitaciones, casos de uso).\n3. Leer un `.parquet` en Colab con Python.\n4. Generar un **diccionario/perfil de variables** del dataset.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) ¿Qué es un archivo `.parquet`?\n\n**Parquet** es un formato de almacenamiento de datos **columnar** (guarda los datos por columnas) y normalmente **comprimido**.  \nFue diseñado para **analítica** y **Big Data**: permite leer rápido, ahorrar espacio y mantener tipos de datos.\n\n### ¿Por qué “columnar” importa?\n- En formatos como CSV/Excel se almacenan datos principalmente **por filas**.\n- En Parquet se guardan por **columnas**, lo cual permite:\n  - Leer **solo** las columnas necesarias (mejor rendimiento).\n  - Comprimir mejor (archivos más pequeños).\n  - Procesar más rápido en motores analíticos (Spark, SQL engines, Python).\n\n### ¿Para qué sirve Parquet?\n- **Data Lakes** (S3/ADLS/GCS): almacenamiento analítico eficiente.\n- Pipelines ETL/ELT: guardar “capas” de datos limpios/curados.\n- Analítica y BI: acelerar agregaciones (KPI), joins y filtros.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Diferencias con otros formatos (XLSX, CSV, etc.)\n\n### XLSX (Excel)\n- ✅ Muy amigable: hojas, tablas, filtros, fórmulas.\n- ✅ Ideal para trabajo manual, reportes y presentación.\n- ❌ No está pensado para Big Data: se vuelve pesado/lento en volúmenes grandes.\n- ❌ No es el mejor formato para automatizar pipelines.\n\n### CSV\n- ✅ Universal: se abre en casi cualquier herramienta.\n- ✅ Útil para intercambio rápido.\n- ❌ Menos eficiente: ocupa más espacio que Parquet y es más lento de leer en analítica.\n- ❌ Tipos de datos frágiles: fechas/números pueden venir como texto.\n\n### PARQUET\n- ✅ Columnar + comprimido: más rápido y compacto.\n- ✅ Mantiene mejor los tipos de datos.\n- ✅ Excelente para analítica y Big Data (Spark/Python/SQL engines).\n- ❌ No se abre “bonito” en Excel sin herramientas adicionales.\n\n**Regla práctica:**\n- Trabajo manual → **XLSX**\n- Intercambio simple → **CSV**\n- Analítica eficiente / Big Data → **PARQUET**\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Preparación del entorno (instalar librerías)\n\nEn Colab, normalmente solo necesitas:\n- `pandas` (tablas)\n- `pyarrow` (lectura/escritura Parquet)\n\nEjecuta la celda siguiente.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Instalación (Colab)\n!pip -q install pyarrow pandas\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Cargar tu archivo Parquet en Colab\n\nSube el archivo `.parquet` desde tu computadora.  \nLuego, el notebook detectará el archivo subido dentro de `/content/`.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from google.colab import files\nuploaded = files.upload()  # Selecciona tu archivo .parquet desde tu PC\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Leer el archivo `.parquet` con pandas\n\nLa celda siguiente:\n1. Busca el primer `.parquet` en `/content`\n2. Lo lee con `pandas.read_parquet`\n3. Muestra tamaño del dataset y primeras filas\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nimport os\n\nparquets = [f for f in os.listdir(\"/content\") if f.lower().endswith(\".parquet\")]\nif not parquets:\n    raise FileNotFoundError(\"No se encontró ningún archivo .parquet en /content. Sube el archivo primero.\")\n\nruta = f\"/content/{parquets[0]}\"\nprint(\"Archivo detectado:\", ruta)\n\ndf = pd.read_parquet(ruta)\nprint(\"Tamaño (filas, columnas):\", df.shape)\n\ndf.head(20)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) ¿Qué variables contiene? (Perfil/Diccionario automático)\n\nGeneraremos una tabla con:\n- Nombre de variable\n- Tipo de dato\n- Nulos y % de nulos\n- # de valores únicos\n- Ejemplos de valores\n- Estadísticos para variables numéricas (min, p25, mediana, promedio, p75, max)\n\nEsto es muy útil como “**diccionario de datos**” inicial.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\n\ndef perfil_variables(df: pd.DataFrame, max_ejemplos=5) -> pd.DataFrame:\n    n = len(df)\n    resumen = []\n\n    for col in df.columns:\n        s = df[col]\n        nulos = int(s.isna().sum())\n        pct_nulos = (nulos / n * 100) if n > 0 else 0.0\n        unicos = int(s.nunique(dropna=True))\n\n        # Ejemplos de valores no nulos\n        ejemplos = s.dropna().astype(str).unique()[:max_ejemplos].tolist()\n        ejemplos_txt = \", \".join(ejemplos) if ejemplos else \"\"\n\n        fila = {\n            \"Variable\": col,\n            \"Tipo\": str(s.dtype),\n            \"Nulos\": nulos,\n            \"% Nulos\": round(pct_nulos, 2),\n            \"Únicos\": unicos,\n            \"Ejemplos\": ejemplos_txt\n        }\n\n        # Si es numérica, agregar estadísticos\n        if pd.api.types.is_numeric_dtype(s):\n            # Proteger si toda la columna es nula\n            if nulos < n:\n                fila.update({\n                    \"Min\": float(np.nanmin(s)),\n                    \"P25\": float(np.nanpercentile(s, 25)),\n                    \"Mediana\": float(np.nanpercentile(s, 50)),\n                    \"Promedio\": float(np.nanmean(s)),\n                    \"P75\": float(np.nanpercentile(s, 75)),\n                    \"Max\": float(np.nanmax(s))\n                })\n            else:\n                fila.update({\"Min\": np.nan, \"P25\": np.nan, \"Mediana\": np.nan, \"Promedio\": np.nan, \"P75\": np.nan, \"Max\": np.nan})\n        else:\n            fila.update({\"Min\": \"\", \"P25\": \"\", \"Mediana\": \"\", \"Promedio\": \"\", \"P75\": \"\", \"Max\": \"\"})\n\n        resumen.append(fila)\n\n    return pd.DataFrame(resumen)\n\nperfil = perfil_variables(df)\nperfil\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Resumen ejecutivo del dataset\n\nMostramos:\n- Filas y columnas\n- Top 10 variables con más nulos\n- Conteo de tipos de datos\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "print(\"=== RESUMEN EJECUTIVO ===\")\nprint(f\"Filas: {df.shape[0]:,}\")\nprint(f\"Columnas: {df.shape[1]:,}\")\n\ntop_nulos = perfil.sort_values(\"% Nulos\", ascending=False).head(10)[[\"Variable\", \"% Nulos\", \"Nulos\"]]\nprint(\"\\nTop 10 variables con más nulos:\")\ndisplay(top_nulos)\n\ntipos = perfil[\"Tipo\"].value_counts()\nprint(\"\\nTipos de datos (conteo de columnas):\")\ndisplay(tipos)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) (Opcional) Detección de columnas tipo fecha y conversión\n\nEn algunos datasets, las fechas llegan como texto.  \nEsta celda intenta convertir columnas `object` a fecha si al menos el **80%** de valores calzan como fecha.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def intentar_convertir_fechas(df: pd.DataFrame, umbral_exito=0.8):\n    df2 = df.copy()\n    convertidas = []\n    for col in df2.columns:\n        if df2[col].dtype == \"object\":\n            parsed = pd.to_datetime(df2[col], errors=\"coerce\", dayfirst=True)\n            exito = parsed.notna().mean()  # proporción parseable\n            if exito >= umbral_exito:\n                df2[col] = parsed\n                convertidas.append((col, round(exito*100, 1)))\n    return df2, convertidas\n\ndf_conv, fechas = intentar_convertir_fechas(df)\n\nif fechas:\n    print(\"Columnas convertidas a fecha (columna, % éxito):\")\n    for c, p in fechas:\n        print(\"-\", c, \":\", p, \"%\")\nelse:\n    print(\"No se detectaron columnas de fecha para convertir automáticamente.\")\n\ndf_conv.dtypes\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) (Opcional) Exportar el diccionario de datos a Excel/CSV\n\nEsto es útil para entregar a estudiantes o documentar el proyecto.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Guardar el perfil/diccionario de variables\nperfil.to_csv(\"diccionario_variables.csv\", index=False)\nperfil.to_excel(\"diccionario_variables.xlsx\", index=False)\n\nprint(\"Archivos generados:\")\nprint(\"- diccionario_variables.csv\")\nprint(\"- diccionario_variables.xlsx\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Cierre: ¿Cuándo usar cada formato?\n\n- **XLSX**: reportes, trabajo manual, presentaciones.\n- **CSV**: intercambio simple y universal.\n- **PARQUET**: analítica eficiente, Big Data, Data Lakes, procesamiento con Spark/SQL/Python.\n\n### Flujo típico Big Data\n1) Fuente (ERP/CRM/IoT) → extracción (CSV/JSON)\n2) Limpieza/transformación → guardado en **Parquet** (Data Lake)\n3) Analítica (Spark/Python/SQL) → resultados (tablas agregadas) → BI\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "Parquet_en_BigData_Guia.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}